{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import lil_matrix,csr_matrix\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the Data\n",
    "\n",
    "### data is taken from https://www.kaggle.com/aiswaryaramachandran/hindienglish-corpora\n",
    "### data have three column source, english_sentence, hindi_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Hindi_English_Truncated_Corpus.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extracting details about data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127607, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 127607 entries, 0 to 127606\n",
      "Data columns (total 3 columns):\n",
      " #   Column            Non-Null Count   Dtype \n",
      "---  ------            --------------   ----- \n",
      " 0   source            127607 non-null  object\n",
      " 1   english_sentence  127605 non-null  object\n",
      " 2   hindi_sentence    127607 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 2.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dropping na values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127605, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dropping duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124827, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ted</td>\n",
       "      <td>politicians do not have permission to do what ...</td>\n",
       "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ted</td>\n",
       "      <td>I'd like to tell you about one such child,</td>\n",
       "      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>This percentage is even greater than the perce...</td>\n",
       "      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ted</td>\n",
       "      <td>what we really mean is that they're bad at not...</td>\n",
       "      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>.The ending portion of these Vedas is called U...</td>\n",
       "      <td>इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      source                                   english_sentence  \\\n",
       "0        ted  politicians do not have permission to do what ...   \n",
       "1        ted         I'd like to tell you about one such child,   \n",
       "2  indic2012  This percentage is even greater than the perce...   \n",
       "3        ted  what we really mean is that they're bad at not...   \n",
       "4  indic2012  .The ending portion of these Vedas is called U...   \n",
       "\n",
       "                                      hindi_sentence  \n",
       "0  राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...  \n",
       "1  मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...  \n",
       "2   यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।  \n",
       "3     हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते  \n",
       "4        इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'जहां तक गंधक के अन्य उत्पादों का प्रश्न है , देश में लोहे , तांबे , सोडियम , सल्फेट आदि की आवश्यकता सीमित थी और युद्धोपरांत हुआ उत्पादन सामान्य रूप से पर्याप्त था .'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['hindi_sentence'].values[124824]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'श्न'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['hindi_sentence'].values[124824][36]+df['hindi_sentence'].values[124824][37]+df['hindi_sentence'].values[124824][38]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doing some preprocessing on text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess(data_x):\n",
    "    data1=[]\n",
    "    for data in tqdm(data_x):\n",
    "        \n",
    "        data=data.replace('\\\\r','').replace('\\\\n','').replace('\\\\',' ').replace(\"n\\'t\",' not').replace(',',' ')\\\n",
    "        .replace('.',' ').replace('%',' ').replace(\"'s\",' is').replace('-',' ').replace('\"','').replace('_','').replace(':','')\\\n",
    "        .replace(';','').replace('!','').replace('!!','').replace(';','').replace('/',' ').replace('?',' ').replace('  ',' ')\\\n",
    "        .replace('।','')\n",
    "\n",
    "        p = re.compile('[0-9]+')\n",
    "        data=p.sub('', data)\n",
    "\n",
    "        data = re.sub(r\"won't\", \"will not\", data)\n",
    "        data = re.sub(r\"can\\'t\", \"can not\", data)\n",
    "\n",
    "        # general\n",
    "        data = re.sub(r\"n\\'t\", \" not\", data)\n",
    "        data = re.sub(r\"\\'re\", \" are\", data)\n",
    "        data = re.sub(r\"\\'s\", \" is\", data)\n",
    "        data = re.sub(r\"\\'d\", \" would\", data)\n",
    "        data = re.sub(r\"\\'ll\", \" will\", data)\n",
    "        data = re.sub(r\"\\'t\", \" not\", data)\n",
    "        data = re.sub(r\"\\'ve\", \" have\", data)\n",
    "        data = re.sub(r\"\\'m\", \" am\", data)\n",
    "\n",
    "#         data = ' '.join(e for e in data.split() if e.lower() not in stopwords)\n",
    "        data=data.lower()\n",
    "        data=' '.join(data.split())\n",
    "        data1.append(data)\n",
    "    return data1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 124827/124827 [00:02<00:00, 49226.21it/s]\n"
     ]
    }
   ],
   "source": [
    "df['english_sentence']=preprocess(df['english_sentence'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ted</td>\n",
       "      <td>politicians do not have permission to do what ...</td>\n",
       "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ted</td>\n",
       "      <td>i would like to tell you about one such child</td>\n",
       "      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>this percentage is even greater than the perce...</td>\n",
       "      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ted</td>\n",
       "      <td>what we really mean is that they are bad at no...</td>\n",
       "      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>the ending portion of these vedas is called up...</td>\n",
       "      <td>इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      source                                   english_sentence  \\\n",
       "0        ted  politicians do not have permission to do what ...   \n",
       "1        ted      i would like to tell you about one such child   \n",
       "2  indic2012  this percentage is even greater than the perce...   \n",
       "3        ted  what we really mean is that they are bad at no...   \n",
       "4  indic2012  the ending portion of these vedas is called up...   \n",
       "\n",
       "                                      hindi_sentence  \n",
       "0  राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...  \n",
       "1  मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...  \n",
       "2   यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।  \n",
       "3     हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते  \n",
       "4        इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 124827/124827 [00:02<00:00, 42139.20it/s]\n"
     ]
    }
   ],
   "source": [
    "df['hindi_sentence']=preprocess(df['hindi_sentence'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['ted',\n",
       "        'politicians do not have permission to do what needs to be done',\n",
       "        'राजनीतिज्ञों के पास जो कार्य करना चाहिए वह करने कि अनुमति नहीं है'],\n",
       "       ['ted', 'i would like to tell you about one such child',\n",
       "        'मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहूंगी'],\n",
       "       ['indic2012',\n",
       "        'this percentage is even greater than the percentage in india',\n",
       "        'यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है'],\n",
       "       ['ted',\n",
       "        'what we really mean is that they are bad at not paying attention',\n",
       "        'हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते'],\n",
       "       ['indic2012',\n",
       "        'the ending portion of these vedas is called upanishad',\n",
       "        'इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है']], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keeping sentence which have only less than 25 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "124827it [00:00, 463498.95it/s]\n"
     ]
    }
   ],
   "source": [
    "keeping_data=[]\n",
    "for i,val in tqdm(enumerate(df.values)):\n",
    "    #print(i,val)\n",
    "    if len(val[1].split(' '))<=25:\n",
    "        keeping_data.append(val)\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105129"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keeping_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(keeping_data,columns=['source','english_sentence','hindi_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105129, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "105129it [00:00, 444769.78it/s]\n"
     ]
    }
   ],
   "source": [
    "keeping_data=[]\n",
    "for i,val in tqdm(enumerate(df.values)):\n",
    "    #print(i,val)\n",
    "    if len(val[2].split(' '))<=25:\n",
    "        keeping_data.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(keeping_data,columns=['source','english_sentence','hindi_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97838, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoding text so that we can input it in encoder-decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['decoder_input_hin']='<start> '+df['hindi_sentence']\n",
    "df['hindi_sentence']='<start> '+df['hindi_sentence']+' <end>'\n",
    "df['english_sentence']=df['english_sentence']+' eos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ted</td>\n",
       "      <td>politicians do not have permission to do what ...</td>\n",
       "      <td>&lt;start&gt; राजनीतिज्ञों के पास जो कार्य करना चाहि...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ted</td>\n",
       "      <td>i would like to tell you about one such child eos</td>\n",
       "      <td>&lt;start&gt; मई आपको ऐसे ही एक बच्चे के बारे में बत...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>this percentage is even greater than the perce...</td>\n",
       "      <td>&lt;start&gt; यह प्रतिशत भारत में हिन्दुओं प्रतिशत स...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ted</td>\n",
       "      <td>what we really mean is that they are bad at no...</td>\n",
       "      <td>&lt;start&gt; हम ये नहीं कहना चाहते कि वो ध्यान नहीं...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>the ending portion of these vedas is called up...</td>\n",
       "      <td>&lt;start&gt; इन्हीं वेदों का अंतिम भाग उपनिषद कहलात...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      source                                   english_sentence  \\\n",
       "0        ted  politicians do not have permission to do what ...   \n",
       "1        ted  i would like to tell you about one such child eos   \n",
       "2  indic2012  this percentage is even greater than the perce...   \n",
       "3        ted  what we really mean is that they are bad at no...   \n",
       "4  indic2012  the ending portion of these vedas is called up...   \n",
       "\n",
       "                                      hindi_sentence  \n",
       "0  <start> राजनीतिज्ञों के पास जो कार्य करना चाहि...  \n",
       "1  <start> मई आपको ऐसे ही एक बच्चे के बारे में बत...  \n",
       "2  <start> यह प्रतिशत भारत में हिन्दुओं प्रतिशत स...  \n",
       "3  <start> हम ये नहीं कहना चाहते कि वो ध्यान नहीं...  \n",
       "4  <start> इन्हीं वेदों का अंतिम भाग उपनिषद कहलात...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### splitting the data in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df=df[['english_sentence','hindi_sentence']]\n",
    "# y=df['hindi_sentence']\n",
    "train, test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70532</th>\n",
       "      <td>the question then is what would that actually ...</td>\n",
       "      <td>&lt;start&gt; प्र्शन यह है कि इसका समय संद्र्भ के लि...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46432</th>\n",
       "      <td>sita is a devotee of her husband eos</td>\n",
       "      <td>&lt;start&gt; सीता का पातिव्रत महान है &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96546</th>\n",
       "      <td>the number stanzas in skandha puran is eighty ...</td>\n",
       "      <td>&lt;start&gt; स्कन्धपुराण में श्लोकों की संख्या इक्य...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58775</th>\n",
       "      <td>in the main room there are the false tombs of ...</td>\n",
       "      <td>&lt;start&gt; मुख्य कक्ष में मुमताज महल एवं शाहजहाँ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9014</th>\n",
       "      <td>so we have developed certain tactics and strat...</td>\n",
       "      <td>&lt;start&gt; तो हमने इस अंतर को कम करने के लिए &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68828</th>\n",
       "      <td>not any longer though eos</td>\n",
       "      <td>&lt;start&gt; लेकिन अब ऐसा नहीं हो रहा है &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14794</th>\n",
       "      <td>okay so let is have a look at a little bit of ...</td>\n",
       "      <td>&lt;start&gt; चलिये डेटा के एक हिस्से पर नजर डालते ह...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14565</th>\n",
       "      <td>however changesin the law may make the leaflet...</td>\n",
       "      <td>&lt;start&gt; लेकिन कायद म क गई बदलाव से इस पुस्तिका...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51055</th>\n",
       "      <td>lrb more details are in the leaflet how to get...</td>\n",
       "      <td>&lt;start&gt; ( इसके बारें में अधिक जानकारी “ सोशल स...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63871</th>\n",
       "      <td>want to do something about it eos</td>\n",
       "      <td>&lt;start&gt; कैसे आप उन्हें कुछ करने के लिये प्रेरि...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78270 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        english_sentence  \\\n",
       "70532  the question then is what would that actually ...   \n",
       "46432               sita is a devotee of her husband eos   \n",
       "96546  the number stanzas in skandha puran is eighty ...   \n",
       "58775  in the main room there are the false tombs of ...   \n",
       "9014   so we have developed certain tactics and strat...   \n",
       "...                                                  ...   \n",
       "68828                          not any longer though eos   \n",
       "14794  okay so let is have a look at a little bit of ...   \n",
       "14565  however changesin the law may make the leaflet...   \n",
       "51055  lrb more details are in the leaflet how to get...   \n",
       "63871                  want to do something about it eos   \n",
       "\n",
       "                                          hindi_sentence  \n",
       "70532  <start> प्र्शन यह है कि इसका समय संद्र्भ के लि...  \n",
       "46432             <start> सीता का पातिव्रत महान है <end>  \n",
       "96546  <start> स्कन्धपुराण में श्लोकों की संख्या इक्य...  \n",
       "58775  <start> मुख्य कक्ष में मुमताज महल एवं शाहजहाँ ...  \n",
       "9014     <start> तो हमने इस अंतर को कम करने के लिए <end>  \n",
       "...                                                  ...  \n",
       "68828          <start> लेकिन अब ऐसा नहीं हो रहा है <end>  \n",
       "14794  <start> चलिये डेटा के एक हिस्से पर नजर डालते ह...  \n",
       "14565  <start> लेकिन कायद म क गई बदलाव से इस पुस्तिका...  \n",
       "51055  <start> ( इसके बारें में अधिक जानकारी “ सोशल स...  \n",
       "63871  <start> कैसे आप उन्हें कुछ करने के लिये प्रेरि...  \n",
       "\n",
       "[78270 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_enc_inp = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "sd=t_enc_inp.fit_on_texts(train['english_sentence'].values)\n",
    "\n",
    "encoded_docs_train = t_enc_inp.texts_to_sequences(train['english_sentence'].values)\n",
    "encoded_docs_test = t_enc_inp.texts_to_sequences(test['english_sentence'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "length=[len(i) for i in encoded_docs_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3989., 11499., 17370., 10931., 12715.,  8862.,  4561.,  4974.,\n",
       "         2855.,   514.]),\n",
       " array([ 1. ,  3.7,  6.4,  9.1, 11.8, 14.5, 17.2, 19.9, 22.6, 25.3, 28. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_seq_train = pad_sequences(encoded_docs_train, maxlen=399, dtype='int32', padding='post')\n",
    "encoder_seq_test = pad_sequences(encoded_docs_test, maxlen=399, dtype='int32', padding='post')\n",
    "# decoder_out_seq = pad_sequences(encoded_out, maxlen=371, dtype='int32', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  344,     6,  5731, ...,     0,     0,     0],\n",
       "       [    4,   609,    10, ...,     0,     0,     0],\n",
       "       [   69,   122,   893, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [    1, 20821,    17, ...,     0,     0,     0],\n",
       "       [  706,     1,  7810, ...,     0,     0,     0],\n",
       "       [   21,  3384,  3278, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_seq_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "del encoded_docs_train, encoded_docs_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## making embded matrix for english text data and for hindi text we will use trainable embedding layer in network itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('D://glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.66432   0.012997  0.01181  -0.084488 -0.78247   1.1457    0.25839\n",
      " -0.6626   -1.1347   -0.83052   1.4438   -0.17832   0.52204   0.087625\n",
      "  0.098307 -0.70804   0.058561  0.40312   0.72853   0.15914   0.0672\n",
      "  0.016688 -0.52523   0.85851   0.85145  -0.11051   1.0307   -0.17542\n",
      " -0.2927   -0.59956  -0.24308   1.2302   -0.81797   0.46934   0.20564\n",
      " -0.25792   0.77917  -0.60109   1.4014   -0.5972   -0.40991   0.38727\n",
      "  0.027353 -0.054872 -0.17931  -0.99247   0.27762  -0.20651  -0.72933\n",
      "  0.35249  -0.089301  0.81046   0.95734  -0.49469  -0.41374   0.71049\n",
      " -0.8651    1.2328    0.14062  -1.0762    0.45116   0.20948  -0.49114\n",
      " -0.091184 -1.1643    0.3087   -0.78891  -0.50382  -0.089517  0.44128\n",
      "  0.83588  -0.089127  0.31822  -0.59513  -0.28502  -1.7288   -0.69524\n",
      " -0.023866 -0.7321   -0.5406    0.081562  0.063065  1.4587   -0.017455\n",
      "  0.11111   0.71711   0.21097  -0.22816   0.57031   1.2428    0.34171\n",
      "  0.31544   0.18129   0.15407  -0.094941  0.049125  0.68225  -0.14102\n",
      "  1.2634   -0.44775 ]\n"
     ]
    }
   ],
   "source": [
    "sd=embeddings_index.get('eos')\n",
    "print(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_inp_vocab_size=max(t_enc_inp.index_word.keys())\n",
    "embedding_matrix = np.zeros((eng_inp_vocab_size+1, 100))\n",
    "for word, i in t_enc_inp.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41168, 100)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dec_out = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "sd=t_dec_out.fit_on_texts(train['hindi_sentence'].values)\n",
    "\n",
    "encoded_docs_train = t_dec_out.texts_to_sequences(train['hindi_sentence'].values)\n",
    "# encoded_docs_test = t_dec_out.texts_to_sequences(test['hindi_sentence'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45243"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t_dec_out.index_word.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_hindi=max(t_dec_out.index_word.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45243"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(t_dec_out.index_word.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "length=[len(i) for i in encoded_docs_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 4027.,  9502., 15061., 10208., 12603.,  9464.,  5118.,  6360.,\n",
       "         4763.,  1164.]),\n",
       " array([ 2. ,  4.7,  7.4, 10.1, 12.8, 15.5, 18.2, 20.9, 23.6, 26.3, 29. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## making dataloader to inputing input_encoder, input_decoder and output_decoder to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data, embedding,tokenizer_eng,tokenizer_hin,max_len,vocab_hindi):\n",
    "        self.encoder_inps = data['english_sentence'].values\n",
    "#         self.decoder_inps = data['decoder_input_hin'].values\n",
    "        self.decoder_outs = data['hindi_sentence'].values\n",
    "        self.embedding = embedding\n",
    "#         self.embedding1 = embedding1\n",
    "        self.tknizer_eng = tokenizer_eng\n",
    "#         self.tknizer_hin_inp = tokenizer_hin_inp\n",
    "        self.tknizer_hin = tokenizer_hin\n",
    "        self.max_len = max_len\n",
    "        self.vocab_hindi=vocab_hindi\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.encoder_seq = self.tknizer_eng.texts_to_sequences([self.encoder_inps[i]]) # need to pass list of values\n",
    "#         self.decoder_inp_seq = self.tknizer_hin_inp.texts_to_sequences([self.decoder_inps[i]])\n",
    "        self.decoder_inp_seq = self.tknizer_hin.texts_to_sequences([self.decoder_outs[i]])\n",
    "#         print(self.decoder_inp_seq)\n",
    "        self.encoder_seq = pad_sequences(self.encoder_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
    "        self.decoder_inp_seq1 = pad_sequences([self.decoder_inp_seq[0][0:len(self.decoder_inp_seq[0])-1]], maxlen=25, dtype='int32', padding='post')\n",
    "        self.decoder_out_seq = pad_sequences([self.decoder_inp_seq[0][1:]], maxlen=25, dtype='int32', padding='post')\n",
    "        #print(self.decoder_inp_seq1)\n",
    "        #print(self.decoder_out_seq)\n",
    "        #print(self.encoder_seq)\n",
    "        self.encoder_seq1=[]\n",
    "        for i in self.encoder_seq[0]:\n",
    "            self.encoder_seq1.append(self.embedding[i])\n",
    "        #print(np.array(encoder_seq1).shape)    \n",
    "        \n",
    "#         decoder_inp_seq1 =[]\n",
    "        \n",
    "#         for i in self.decoder_inp_seq[0]:\n",
    "#             decoder_inp_seq1.append(self.embedding1[i])\n",
    "            \n",
    "#         self.decoder_out_seq1 = np.zeros((25,self.vocab_hindi+1),dtype=np.float32)\n",
    "        \n",
    "#         for i,d in enumerate(self.decoder_out_seq[0]):\n",
    "#             self.decoder_out_seq1[i,d]=1\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(self.encoder_seq.shape)\n",
    "        return np.array(self.encoder_seq1), self.decoder_inp_seq1, self.decoder_out_seq\n",
    "\n",
    "    def __len__(self): # your model.fit_gen requires this function\n",
    "        return len(self.encoder_inps)\n",
    "\n",
    "    \n",
    "class Dataloder(tf.keras.utils.Sequence):    \n",
    "    def __init__(self, dataset, batch_size=1):\n",
    "        #print(dataset)\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.dataset.encoder_inps))\n",
    "        #print(self.indexes)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        #print(i,'in dataloader')\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        data = []\n",
    "        for j in range(start, stop):\n",
    "            #print(self.dataset[j])\n",
    "            data.append(self.dataset[j])\n",
    "            #break\n",
    "        #print(len(data),'in dataloader')\n",
    "        batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n",
    "#         print(batch[2].shape)\n",
    "        # we are creating data like ([italian, english_inp], english_out) these are already converted into seq\n",
    "        return tuple([[batch[0],tf.squeeze(batch[1])],batch[2]])\n",
    "\n",
    "    def __len__(self):  # your model.fit_gen requires this function\n",
    "        return len(self.indexes) // self.batch_size\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.random.permutation(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[[-0.038194  , -0.24487001,  0.72812003, ..., -0.1459    ,\n",
       "            0.82779998,  0.27061999],\n",
       "          [-0.12743001,  0.43527001,  0.86000001, ..., -0.24033   ,\n",
       "            0.37498   ,  0.67258   ],\n",
       "          [-0.094451  , -0.0096052 ,  0.1793    , ...,  0.21709   ,\n",
       "           -0.046804  ,  0.13551   ],\n",
       "          ...,\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ]],\n",
       "  \n",
       "         [[-0.90959001, -0.64754999,  0.15941   , ...,  0.14275999,\n",
       "           -0.096841  , -0.25395   ],\n",
       "          [-0.54263997,  0.41475999,  1.03219998, ..., -1.29690003,\n",
       "            0.76217002,  0.46349001],\n",
       "          [-0.27085999,  0.044006  , -0.02026   , ..., -0.4923    ,\n",
       "            0.63687003,  0.23642001],\n",
       "          ...,\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ]],\n",
       "  \n",
       "         [[-0.038194  , -0.24487001,  0.72812003, ..., -0.1459    ,\n",
       "            0.82779998,  0.27061999],\n",
       "          [-0.45491001,  0.70877999,  0.052253  , ..., -0.42291999,\n",
       "            0.58595002, -0.42407   ],\n",
       "          [-0.26629001,  1.0323    , -0.46836999, ..., -0.88055003,\n",
       "           -0.50164002,  0.49974999],\n",
       "          ...,\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[-0.071953  ,  0.23127   ,  0.023731  , ..., -0.71894997,\n",
       "            0.86894   ,  0.19539   ],\n",
       "          [ 0.60167003,  0.55105001,  0.31797999, ...,  0.12749   ,\n",
       "            0.76086998,  0.44997001],\n",
       "          [-0.41903999,  0.6652    ,  0.29695001, ...,  0.75783998,\n",
       "            0.059711  ,  0.16174001],\n",
       "          ...,\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ]],\n",
       "  \n",
       "         [[ 0.013026  ,  0.33335   ,  0.62812001, ...,  0.10573   ,\n",
       "            0.41712001,  0.80295002],\n",
       "          [ 0.093736  ,  0.56151998,  0.48363999, ..., -0.81304997,\n",
       "            0.62396997,  0.73176003],\n",
       "          [-0.12135   ,  0.15341   , -0.014315  , ..., -0.50936002,\n",
       "            0.5068    ,  0.080349  ],\n",
       "          ...,\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ]],\n",
       "  \n",
       "         [[-0.30664   ,  0.16821   ,  0.98510998, ..., -0.38775   ,\n",
       "            0.36916   ,  0.54521   ],\n",
       "          [ 0.13717   , -0.54286999,  0.19419   , ..., -0.52056998,\n",
       "            0.25433999, -0.23759   ],\n",
       "          [-0.27085999,  0.044006  , -0.02026   , ..., -0.4923    ,\n",
       "            0.63687003,  0.23642001],\n",
       "          ...,\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ]]]),\n",
       "  <tf.Tensor: shape=(32, 25), dtype=int32, numpy=\n",
       "  array([[    1, 25574,    16,     4,    14,   137,    58, 25575,     3,\n",
       "             21,    60,  7256,     4,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,  1351,    10, 10876,   434,     4,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1, 19836,     5,  1303,     6,   361, 19837,   633,    12,\n",
       "           1123,     9,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,   244,   995,     5,  2766,   933,   106,  1857,     6,\n",
       "           6745,  7863,     9,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,    26,   191,    19,  1182,    11,    99,    33,     3,\n",
       "             21,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,    60,    49,   171,   234,  4987,  7864,    11,  8658,\n",
       "           2906,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,    55,    40, 19838,  1137,   121,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,  3143,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,   922,  4195,     7, 19839,   233,    13,    20,  2835,\n",
       "              8,  2279,  9676,  1167,    98,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,   171,   129,    86,    49,    26,  1500,    37,  5553,\n",
       "             35,  1332,     5, 15689,  3319,    46,  3232,    33,   160,\n",
       "           2525,    11,    86,   376,   253,     0,     0],\n",
       "         [    1,  1501,     8,  1624,     3,   164,  3320, 15690,   450,\n",
       "          15691,     9,   338,   227,    10,  1049,  1168,   350,    40,\n",
       "           8659,     4,     0,     0,     0,     0,     0],\n",
       "         [    1,  1517,   463,  1825,    25,    47,     4,    14,  2192,\n",
       "              5,  4196,     7,  5906,     3,  1588,    32,    17,   491,\n",
       "             46,  9677,   181,  5554,    25,    47,     4],\n",
       "         [    1,   703,  1911,     3,    41,    41,  2280, 12398,  3233,\n",
       "             10,  1442,    15,   804,    47,  6296,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,   174, 25576, 25577,     3,     7, 25578,  1696,     3,\n",
       "             93,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,  1389,  9678,   131,   544,     5,    34,    39,  1879,\n",
       "           2427,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,     7,   382,   315,  6746,  1333,    46,   501,     3,\n",
       "           1790,  5555,    74,    20,  3878,     5,  6747,    50,     9,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,    36,   232, 12399,   153,    62,    92,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,   805,     3,  1443,    10,   901, 10877,     6,   204,\n",
       "              5,    27,    47,     4,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,   734,    11,    35,  1270,     5,   658,   321,    10,\n",
       "           2586,  1113,   483,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,  3321,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,   116,    56,   611,    22,  1951,  1138,    10,  4197,\n",
       "          15692,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,    65,   738,     7,  2991,    10,   334,     4,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,    26,   866,  2907,    13,   152,     7, 25579,    95,\n",
       "              6,  1603,    11,    41,   178,   152,   144,    65,    90,\n",
       "          19840,    38,     0,     0,     0,     0,     0],\n",
       "         [    1,  5907,    11,  1369, 25580,  3626,    10,  4036,    18,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,    55,    40,    42,   685,   126,     3,    17,    25,\n",
       "            130,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,  6748,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,  9679,     5,  1183,     3,    41,   121, 15693,  3627,\n",
       "              3,   101,   467,    11,   545,  1016, 15694,    13,   513,\n",
       "            483,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,   117,   107,   232,    61,     8,  5256,  2587,     4,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,   201,   221,  2992,  1772,     4,   276, 25581,   612,\n",
       "              4,  1518, 19841,  2428,   996,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,     7,   560,   215,  4988,     6,   464,     3,    78,\n",
       "              5,    71, 15695,   942,     9,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,  1858,  5257,     3,    54,     5,    49,   370,     4,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,    16,   147,   145,    21,    39,  1952,    18,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]])>],\n",
       " array([[[25574,    16,     4,    14,   137,    58, 25575,     3,    21,\n",
       "             60,  7256,     4,     2,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[ 1351,    10, 10876,   434,     4,     2,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[19836,     5,  1303,     6,   361, 19837,   633,    12,  1123,\n",
       "              9,     2,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  244,   995,     5,  2766,   933,   106,  1857,     6,  6745,\n",
       "           7863,     9,     2,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[   26,   191,    19,  1182,    11,    99,    33,     3,    21,\n",
       "              2,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[   60,    49,   171,   234,  4987,  7864,    11,  8658,  2906,\n",
       "              2,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[   55,    40, 19838,  1137,   121,     2,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[ 3143,     2,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  922,  4195,     7, 19839,   233,    13,    20,  2835,     8,\n",
       "           2279,  9676,  1167,    98,     2,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  171,   129,    86,    49,    26,  1500,    37,  5553,    35,\n",
       "           1332,     5, 15689,  3319,    46,  3232,    33,   160,  2525,\n",
       "             11,    86,   376,   253,     2,     0,     0]],\n",
       " \n",
       "        [[ 1501,     8,  1624,     3,   164,  3320, 15690,   450, 15691,\n",
       "              9,   338,   227,    10,  1049,  1168,   350,    40,  8659,\n",
       "              4,     2,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[ 1517,   463,  1825,    25,    47,     4,    14,  2192,     5,\n",
       "           4196,     7,  5906,     3,  1588,    32,    17,   491,    46,\n",
       "           9677,   181,  5554,    25,    47,     4,     2]],\n",
       " \n",
       "        [[  703,  1911,     3,    41,    41,  2280, 12398,  3233,    10,\n",
       "           1442,    15,   804,    47,  6296,     2,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  174, 25576, 25577,     3,     7, 25578,  1696,     3,    93,\n",
       "              2,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[ 1389,  9678,   131,   544,     5,    34,    39,  1879,  2427,\n",
       "              2,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[    7,   382,   315,  6746,  1333,    46,   501,     3,  1790,\n",
       "           5555,    74,    20,  3878,     5,  6747,    50,     9,     2,\n",
       "              0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[   36,   232, 12399,   153,    62,    92,     2,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  805,     3,  1443,    10,   901, 10877,     6,   204,     5,\n",
       "             27,    47,     4,     2,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  734,    11,    35,  1270,     5,   658,   321,    10,  2586,\n",
       "           1113,   483,     2,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[ 3321,     2,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  116,    56,   611,    22,  1951,  1138,    10,  4197, 15692,\n",
       "              2,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[   65,   738,     7,  2991,    10,   334,     4,     2,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[   26,   866,  2907,    13,   152,     7, 25579,    95,     6,\n",
       "           1603,    11,    41,   178,   152,   144,    65,    90, 19840,\n",
       "             38,     2,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[ 5907,    11,  1369, 25580,  3626,    10,  4036,    18,     2,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[   55,    40,    42,   685,   126,     3,    17,    25,   130,\n",
       "              2,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[ 6748,     2,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[ 9679,     5,  1183,     3,    41,   121, 15693,  3627,     3,\n",
       "            101,   467,    11,   545,  1016, 15694,    13,   513,   483,\n",
       "              2,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  117,   107,   232,    61,     8,  5256,  2587,     4,     2,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  201,   221,  2992,  1772,     4,   276, 25581,   612,     4,\n",
       "           1518, 19841,  2428,   996,     2,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[    7,   560,   215,  4988,     6,   464,     3,    78,     5,\n",
       "             71, 15695,   942,     9,     2,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[ 1858,  5257,     3,    54,     5,    49,   370,     4,     2,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[   16,   147,   145,    21,    39,  1952,    18,     2,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]]]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = Dataset(train, embedding_matrix, t_enc_inp,t_dec_out,25,vocab_hindi)\n",
    "test_dataset  = Dataset(test, embedding_matrix, t_enc_inp,t_dec_out,25,vocab_hindi)\n",
    "\n",
    "train_dataloader = Dataloder(train_dataset, batch_size=32)\n",
    "test_dataloader = Dataloder(test_dataset, batch_size=32)\n",
    "train_dataloader[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 399, 100)\n",
      "(2, 418, 67427)\n",
      "(2, 418, 67427)\n"
     ]
    }
   ],
   "source": [
    "data=[(np.zeros((399,100)),np.zeros((418,67427)),np.zeros((418,67427))),(np.zeros((399,100)),np.zeros((418,67427)),np.zeros((418,67427)))]\n",
    "for i in zip(*data):\n",
    "    print(np.stack(i, axis=0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## making a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 25, 100)\n"
     ]
    }
   ],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = keras.Input(shape=(None, 25))\n",
    "encoder = keras.layers.LSTM(64, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = keras.Input(shape=(25))\n",
    "dec_emb_layer = Embedding(vocab_hindi+1,100,input_length=25)(decoder_inputs)\n",
    "print(dec_emb_layer.shape)\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = keras.layers.LSTM(64, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb_layer, initial_state=encoder_states)\n",
    "decoder_dense = keras.layers.Dense(vocab_hindi+1, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, None, 25)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 25, 100)      4524400     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 64), (None,  23040       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, 25, 64), (No 42240       embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 25, 45244)    2940860     lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 7,530,540\n",
      "Trainable params: 7,530,540\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2445/2445 [==============================] - 483s 198ms/step - loss: 3.5919 - accuracy: 0.1267 - val_loss: 3.0692 - val_accuracy: 0.1241\n",
      "Epoch 2/5\n",
      "2445/2445 [==============================] - 485s 198ms/step - loss: 3.0757 - accuracy: 0.1195 - val_loss: 2.9041 - val_accuracy: 0.1203\n",
      "Epoch 3/5\n",
      "2445/2445 [==============================] - 486s 199ms/step - loss: 2.9061 - accuracy: 0.1169 - val_loss: 2.8019 - val_accuracy: 0.1174\n",
      "Epoch 4/5\n",
      "2445/2445 [==============================] - 487s 199ms/step - loss: 2.7661 - accuracy: 0.1144 - val_loss: 2.7260 - val_accuracy: 0.1154\n",
      "Epoch 5/5\n",
      "2445/2445 [==============================] - 487s 199ms/step - loss: 2.6480 - accuracy: 0.1130 - val_loss: 2.6717 - val_accuracy: 0.1144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f28993cd88>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(\n",
    "    train_dataloader,\n",
    "    epochs=5,\n",
    "    validation_data=test_dataloader,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    Encoder model -- That takes a input sequence and returns output sequence\n",
    "    '''\n",
    "\n",
    "    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
    "        super().__init__()\n",
    "        #Initialize Embedding layer\n",
    "        #Intialize Encoder LSTM layer\n",
    "        self.inp_vocab_size=inp_vocab_size\n",
    "        self.embedding_size=embedding_size\n",
    "        self.lstm_size=lstm_size\n",
    "        self.input_length=input_length\n",
    "        \n",
    "        self.embedding=Embedding(self.inp_vocab_size,self.embedding_size,input_length=self.input_length)\n",
    "        self.lstm=LSTM(self.lstm_size,return_state=True,return_sequences=True)\n",
    "\n",
    "    def call(self,input_sequence,states):\n",
    "      '''\n",
    "          This function takes a sequence input and the initial states of the encoder.\n",
    "          Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to encoder_lstm\n",
    "          returns -- All encoder_outputs, last time steps hidden and cell state\n",
    "      '''\n",
    "      embedding1=self.embedding(input_sequence)\n",
    "      # print('embedding in encoder---',embedding1.shape)\n",
    "      self.lstm1,self.h_state,self.c_state=self.lstm(embedding1)\n",
    "      # print('self.lstm1 in encoder---',self.lstm1.shape)\n",
    "      # print('self.h_state in encoder---',self.h_state.shape)\n",
    "      # print('self.c_state in encoder---',self.c_state.shape)\n",
    "      return self.lstm1,self.h_state,self.c_state\n",
    "\n",
    "    \n",
    "    def initialize_states(self,batch_size):\n",
    "      '''\n",
    "      Given a batch size it will return intial hidden state and intial cell state.\n",
    "      If batch size is 32- Hidden state is zeros of size [32,lstm_units], cell state zeros is of size [32,lstm_units]\n",
    "      '''\n",
    "      self.hid_state=np.zeros((batch_size,self.lstm_size))\n",
    "      self.cell_state=np.zeros((batch_size,self.lstm_size))\n",
    "      return self.hid_state,self.cell_state\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "  '''\n",
    "    Class the calculates score based on the scoring_function using Bahdanu attention mechanism.\n",
    "  '''\n",
    "  def __init__(self,scoring_function, att_units):\n",
    "    super().__init__()\n",
    "\n",
    "    # Please go through the reference notebook and research paper to complete the scoring functions\n",
    "    self.att_units=att_units\n",
    "    self.scoring_function=scoring_function\n",
    "    if self.scoring_function=='dot':\n",
    "      # Intialize variables needed for Dot score function here\n",
    "      # self.softmax=Softmax(axis=1)\n",
    "      pass\n",
    "\n",
    "      \n",
    "      \n",
    "    if scoring_function == 'general':\n",
    "      # Intialize variables needed for General score function here\n",
    "      self.dense1=Dense(self.att_units,activation=None)\n",
    "      # self.softmax1=Softmax(axis=1)\n",
    "      \n",
    "    elif scoring_function == 'concat':\n",
    "      # Intialize variables needed for Concat score function here\n",
    "      self.dense2=Dense(self.att_units,activation=None)\n",
    "      self.dense3=Dense(self.att_units,activation=None)\n",
    "      self.dense4=Dense(1,activation=None)\n",
    "      \n",
    "  \n",
    "  \n",
    "  def call(self,decoder_hidden_state,encoder_output):\n",
    "    '''\n",
    "      Attention mechanism takes two inputs current step -- decoder_hidden_state and all the encoder_outputs.\n",
    "      * Based on the scoring function we will find the score or similarity between decoder_hidden_state and encoder_output.\n",
    "        Multiply the score function with your encoder_outputs to get the context vector.\n",
    "        Function returns context vector and attention weights(softmax - scores)\n",
    "    '''\n",
    "    decoder_hidden_state = tf.expand_dims(decoder_hidden_state, axis=1)\n",
    "    if self.scoring_function == 'dot':\n",
    "        score = tf.matmul(decoder_hidden_state, encoder_output, transpose_b=True)\n",
    "        # print(score.shape,'score')\n",
    "        score = tf.transpose(score,[0,2,1])\n",
    "        # print(score.shape,'score')\n",
    "        attention_weights = tf.math.softmax(score,axis=1)\n",
    "        # print(attention_weights.shape,'attention_weights')\n",
    "        \n",
    "        \n",
    "        # print(encoder_output.shape,'encoder_output')\n",
    "        # print(decoder_hidden_state.shape,'decoder_hidden_state')\n",
    "        \n",
    "        \n",
    "        # print(attention_weights.shape,'attention_weights')\n",
    "        context_vector=tf.matmul(attention_weights,encoder_output, transpose_a=True)\n",
    "        # print(context_vector.shape,'context_vector')\n",
    "        # context_vector = tf.transpose(context_vector,[0,2,1])\n",
    "        # print(context_vector.shape,'context_vector')\n",
    "        context_vector=tf.squeeze(context_vector,axis=1)\n",
    "        # print(context_vector.shape,'context_vector')\n",
    "        \n",
    "          \n",
    "        \n",
    "        return context_vector,attention_weights\n",
    "\n",
    "        \n",
    "    elif self.scoring_function == 'general':\n",
    "        \n",
    "        output1=self.dense1(encoder_output)\n",
    "        score = tf.matmul(output1,decoder_hidden_state, transpose_b=True)\n",
    "        # score = tf.transpose(score,[0,2,1])\n",
    "        # print(score.shape,'score')\n",
    "        attention_weights = tf.math.softmax(score,axis=1)\n",
    "        context_vector=tf.matmul(attention_weights,encoder_output, transpose_a=True)\n",
    "        #context_vector = tf.transpose(context_vector,[0,2,1])\n",
    "        context_vector=tf.squeeze(context_vector,axis=1)\n",
    "          \n",
    "        return context_vector,attention_weights\n",
    "\n",
    "    \n",
    "    elif self.scoring_function == 'concat':\n",
    "    #     # Implement General score function here\n",
    "          concat1=self.dense2(encoder_output)\n",
    "          concat2=self.dense3(decoder_hidden_state)\n",
    "          # concat2=tf.repeat(concat2,)\n",
    "          concat1=tf.transpose(concat1,[1,0,2])\n",
    "          concat2=tf.transpose(concat2,[1,0,2])\n",
    "          # print(concat1.shape,concat2.shape)\n",
    "          #concat = tf.map_fn(lambda x: tf.add(x, concat2), concat1)\n",
    "          concat=tf.transpose(tf.multiply(concat2,concat1),[1,0,2])\n",
    "          concat=tf.math.tanh(concat)\n",
    "          attention_weights=self.dense4(concat)\n",
    "          # print(attention_weights.shape)\n",
    "          context_vector=tf.matmul(attention_weights,encoder_output, transpose_a=True)\n",
    "          #context_vector = tf.transpose(context_vector,[0,2,1])\n",
    "          context_vector=tf.squeeze(context_vector,axis=1)\n",
    "          # print(context_vector.shape,'context_vector')\n",
    "          return context_vector,attention_weights\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class One_Step_Decoder(tf.keras.Model):\n",
    "  def __init__(self,tar_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
    "\n",
    "      # Initialize decoder embedding layer, LSTM and any other objects needed\n",
    "      super().__init__()\n",
    "      #Initialize Embedding layer\n",
    "      #Intialize Encoder LSTM layer\n",
    "      self.inp_vocab_size=tar_vocab_size\n",
    "      self.embedding_size=embedding_dim\n",
    "      self.lstm_size=dec_units\n",
    "      self.input_length=input_length\n",
    "      self.att_units=att_units\n",
    "      self.score_fun=score_fun\n",
    "      \n",
    "      self.embedding=Embedding(self.inp_vocab_size,self.embedding_size,input_length=self.input_length)\n",
    "      self.lstm=LSTM(self.lstm_size,return_state=True,return_sequences=True)\n",
    "      self.attention=Attention(self.score_fun,self.att_units)\n",
    "      self.dense=Dense(self.inp_vocab_size,activation='relu')\n",
    "      self.concat=tf.keras.layers.Concatenate()\n",
    "\n",
    "\n",
    "  def call(self,input_to_decoder, encoder_output, state_h,state_c):\n",
    "    '''\n",
    "        One step decoder mechanisim step by step:\n",
    "      A. Pass the input_to_decoder to the embedding layer and then get the output(batch_size,1,embedding_dim)\n",
    "      B. Using the encoder_output and decoder hidden state, compute the context vector.\n",
    "      C. Concat the context vector with the step A output\n",
    "      D. Pass the Step-C output to LSTM/GRU and get the decoder output and states(hidden and cell state)\n",
    "      E. Pass the decoder output to dense layer(vocab size) and store the result into output.\n",
    "      F. Return the states from step D, output from Step E, attention weights from Step -B\n",
    "\n",
    "    '''\n",
    "    embedding_out=self.embedding(input_to_decoder)\n",
    "    \n",
    "    self.context_vector,self.attention_weights=self.attention(state_h,encoder_output)\n",
    "    \n",
    "    concat_vec=self.concat((embedding_out,tf.expand_dims(self.context_vector,axis=1)))\n",
    "    \n",
    "    self.lstm1,self.h_state,self.c_state=self.lstm(concat_vec,initial_state=[state_h,state_c])\n",
    "    #print(self.lstm1.shape,'self.lstm1.shape')\n",
    "    output=self.dense(self.lstm1)\n",
    "    #print(output.shape,'output.shape')\n",
    "    output=tf.squeeze(output,axis=1)\n",
    "    #print(output.shape,'output.shape')\n",
    "    \n",
    "    return output,self.h_state,self.c_state,self.attention_weights,self.context_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
    "      #Intialize necessary variables and create an object from the class onestepdecoder\n",
    "      super().__init__()\n",
    "      #Initialize Embedding layer\n",
    "      #Intialize Encoder LSTM layer\n",
    "      self.inp_vocab_size=out_vocab_size\n",
    "      self.embedding_dim=embedding_dim\n",
    "      self.lstm_size=dec_units\n",
    "      self.input_length=input_length\n",
    "      self.att_units=att_units\n",
    "      self.score_fun=score_fun\n",
    "\n",
    "      self.onestepdecoder=One_Step_Decoder(self.inp_vocab_size, self.embedding_dim, self.input_length, self.lstm_size ,self.score_fun ,self.att_units)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def call(self, input_to_decoder,encoder_output,decoder_hidden_state,decoder_cell_state ):\n",
    "\n",
    "        #Initialize an empty Tensor array, that will store the outputs at each and every time step\n",
    "        #Create a tensor array as shown in the reference notebook\n",
    "        # print(input_to_decoder.shape,'input_to_decoder.shape')\n",
    "        all_outputs=tf.TensorArray(tf.float32,tf.shape(input_to_decoder)[1],name='output_arrays')\n",
    "        #print(input_to_decoder.shape,'input_to_decoder')\n",
    "        for i in range(tf.shape(input_to_decoder)[1]):\n",
    "          # print(input_to_decoder[:,i:i+1],'input_to_decoder[:,i:i+1]')\n",
    "          output,decoder_hidden_state,decoder_cell_state,attention_weights,context_vector=self.onestepdecoder(input_to_decoder[:,i:i+1],encoder_output,decoder_hidden_state,decoder_cell_state)\n",
    "          all_outputs=all_outputs.write(i,output)\n",
    "        # print(np.array(all_outputs).shape,'np.array(all_outputs).shape')  \n",
    "        all_outputs=tf.transpose(all_outputs.stack(),[1,0,2])  \n",
    "        # print(all_outputs.shape,'all_outputs.shape')  \n",
    "        return all_outputs\n",
    "        \n",
    "        #Iterate till the length of the decoder input\n",
    "            # Call onestepdecoder for each token in decoder_input\n",
    "            # Store the output in tensorarray\n",
    "        # Return the tensor array\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_decoder(tf.keras.Model):\n",
    "  def __init__(self,inp_vocab_size,embedding_size,lstm_size,encoder_input_length,out_vocab_size, decoder_input_length, dec_units ,score_fun ,att_units):\n",
    "\n",
    "    # self,out_vocab_size, decoder_input_length, dec_units ,score_fun ,att_units\n",
    "    #Intialize objects from encoder decoder\n",
    "    super().__init__()\n",
    "    #Create encoder object\n",
    "    #Create decoder object\n",
    "    #Intialize Dense layer(out_vocab_size) with activation='softmax'\n",
    "    # self.embedding=Embedding(out_vocab_size,embedding_size,input_length=decoder_input_length)\n",
    "    self.encoder=Encoder(inp_vocab_size,embedding_size=embedding_size,lstm_size=lstm_size,input_length=encoder_input_length)\n",
    "    self.decoder=Decoder(out_vocab_size,embedding_dim=embedding_size,input_length=decoder_input_length,dec_units=dec_units,score_fun=score_fun,att_units=att_units)\n",
    "    \n",
    "    # self.dense=Dense(output_length,activation='softmax')\n",
    "    \n",
    "  def call(self,data):\n",
    "    input,output=data[0],data[1]\n",
    "    #print(input[0])\n",
    "\n",
    "    #print('Input---',input.shape)\n",
    "    #print('output---',output.shape)\n",
    "    \n",
    "    enc_output, encoder_h, encoder_c=self.encoder(input,'kjl')\n",
    "\n",
    "    # print('enc_output---',enc_output.shape)\n",
    "    # print('encoder_h---',encoder_h.shape)\n",
    "    # print('encoder_c---',encoder_c.shape)\n",
    "\n",
    "    # embeddings1=self.embedding(enc_output)\n",
    "\n",
    "    # print('embeddings1----',embeddings1.shape)\n",
    "\n",
    "    decoder_output=self.decoder(output,enc_output,encoder_h,encoder_c)\n",
    "    \n",
    "    # print('decoder_output---',decoder_output.shape)\n",
    "\n",
    "    return decoder_output\n",
    "    #Intialize encoder states, Pass the encoder_sequence to the embedding layer\n",
    "    # Decoder initial states are encoder final states, Initialize it accordingly\n",
    "    # Pass the decoder sequence,encoder_output,decoder states to Decoder\n",
    "    # return the decoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
